---
title: "Ch12-unsup-lab"
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.11.2
  kernelspec:
    display_name: R
    language: R
    name: ir
---

```{r eval=FALSE,warning = FALSE, message = FALSE}
# used new libraries
if (!require("ISLR2")) install.packages("ISLR2")
if (!require("ggbiplot")) {
  if (!require("devtools")) install.packages("devtools")
  devtools::install_github("vqv/ggbiplot")
}

```

```{r setup, include=FALSE,warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE)
library(plotly)
library(ggbiplot)

```



# Lab: Unsupervised Learning
## Principal Components Analysis

In this lab, we perform PCA on the `USArrests` data set, which is part of the base `R` package.
The rows of the data set contain the 50 states, in alphabetical order.

```{r}
USArrests
```

```{r chunk1}
states <- row.names(USArrests)
states
```

The columns of the data set contain the four variables.

```{r chunk2}
names(USArrests)
```

We first briefly examine the data. We notice that the variables have vastly different means.


```{r chunk3}
cat("Mean:\n")
apply(USArrests, 2, mean)
cat("Var:\n")
apply(USArrests, 2, var)
```

Not surprisingly, the variables also have vastly different variances:
 the `UrbanPop` variable measures the percentage of the population in each state living in an urban area, which is not a comparable number to the number of rapes
in each state per 100,000 individuals.
If we failed to scale the variables before performing PCA, then most of the principal components that we observed would be driven by the `Assault` variable, since it has by far the largest mean and variance.
Thus, it is important to standardize the variables to have mean zero and standard deviation one before performing PCA.

We now perform principal components analysis using the `prcomp()` function, which is one of several functions in `R` that perform PCA.

```{r chunk5}
pr.out <- prcomp(USArrests, scale = TRUE)
pr.out
```

By default, the `prcomp()` function centers the variables to have
mean zero. By using the option `scale = TRUE`, we scale the
variables to have standard deviation one. The output from
`prcomp()` contains a number of useful quantities.

```{r chunk6}
names(pr.out)
```

The `center` and `scale` components correspond to the means and standard deviations of the variables that were used for scaling prior to implementing PCA.

```{r chunk7}
pr.out$center
pr.out$scale
```

The `rotation` matrix provides the principal component loadings;
each column of `pr.out$rotation` contains the corresponding
principal component loading vector. ( *This function names it the rotation matrix, because when we matrix-multiply the $\bf X$   matrix by `pr.out$rotation`, it gives us the coordinates of the   data in the rotated coordinate system. These coordinates are the  principal component scores.* )


```{r chunk8}
pr.out$rotation
```

We see that there are four distinct principal components. This is to
be expected because there are in general $\min(n-1,p)$ informative
principal components in a data set with $n$ observations and $p$
variables.

Using the `prcomp()` function, we do not need to explicitly multiply the data by the principal component loading vectors  in order to obtain the principal component score vectors. Rather the $50 \times 4$ matrix `x` has as its columns the principal component score vectors. That is, the $k$th column is the $k$th principal component score vector.

```{r chunk9}
dim(pr.out$x)
```

We can plot the first two principal components as follows:

```{r chunk10}
biplot(pr.out, scale = 0)
```

The `scale = 0` argument to `biplot()` ensures that the arrows are scaled to represent the loadings; other values for `scale` give slightly different biplots with different interpretations.


Notice that this figure is a mirror image of Figure 12.1. Recall that the principal components are only unique up to a sign change, so we can reproduce Figure 12.1 by making a few small changes:

```{r chunk11}
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale = 0)
```

```{r}
ggbiplot(pr.out, scale = 0, labels=rownames(pr.out$x))
```

```{r}
ggplotly(ggbiplot(pr.out, scale = 0, labels=rownames(pr.out$x)))
```

The `prcomp()` function also outputs the standard deviation of each principal component. For instance, on the `USArrests` data set, we can access these standard deviations as follows:

```{r chunk12}
pr.out$sdev
```

The variance explained by each principal component is obtained by squaring these:

```{r chunk13}
pr.var <- pr.out$sdev^2
pr.var
```

To compute the proportion of variance explained by each principal component, we simply divide the variance explained by each principal component by the total variance explained by all four principal components:

```{r chunk14}
pve <- 100 * pr.var / sum(pr.var)
pve
```

We see that the first principal component explains $62.0\,\%$ of the variance in the data, the next principal component explains $24.7\,\%$ of the variance, and so forth.
 We can plot the PVE explained by each component, as well as the cumulative PVE, as follows:

```{r chunk15}
par(mfrow = c(1, 2))
plot(pve, xlab = "Principal Component",
    ylab = "Proportion of Variance Explained",
    type = "b")
plot(cumsum(pve), xlab = "Principal Component",
    ylab = "Cumulative Proportion of Variance Explained",
     type = "b")
```

 The result is shown in Figure 12.3.
Note that the function `cumsum()` computes the cumulative sum of the elements of  a numeric vector. For instance:

```{r chunk16}
a <- c(1, 2, 8, -3)
cumsum(a)
```




## NCI60 Data Example


Unsupervised techniques are often used in the analysis of genomic data. In particular, PCA and hierarchical clustering are popular tools.
 We  illustrate these techniques on the `NCI` cancer cell line microarray data, which consists of $6{,}830$ gene expression measurements on $64$ cancer cell lines.

```{r chunk41}
library(ISLR2)
nci.labs <- NCI60$labs
nci.data <- NCI60$data
nrow(nci.data)
ncol(nci.data)
nci.data[1:5,1:5]
```

Each cell line is labeled with a cancer type, given in `nci.labs`. We do not make use of the cancer types in performing PCA and clustering, as these are unsupervised techniques. But
after performing PCA and clustering, we will
check to see the extent to which these cancer types agree with the results of these unsupervised techniques.

The data has $64$ rows and $6{,}830$ columns.

```{r chunk42}
dim(nci.data)
```


We begin by examining the cancer types for the cell lines.

```{r chunk43}
nci.labs[1:4]
table(nci.labs)
```


### PCA on the NCI60 Data

We first perform PCA on the data after scaling the variables (genes) to have standard deviation one, although one could reasonably argue that it is better not to scale the genes.

```{r chunk44}
pr.out <- prcomp(nci.data, scale = TRUE)
```

We now  plot the first few principal component score vectors, in order to visualize the data. The observations (cell lines) corresponding to a given cancer type will be plotted in the same color, so that we can see to what extent the observations within a cancer type are similar to each other. We first create a simple function that assigns a distinct color to each element of a numeric vector.
The function will be used to assign a  color to each of the $64$ cell lines, based on the cancer type to which it corresponds.

```{r chunk45}
Cols <- function(vec) {
   cols <- rainbow(length(unique(vec)))
   return(cols[as.numeric(as.factor(vec))])
 }
```

Note that the `rainbow()` function takes as its argument a positive integer, and returns a vector containing that number of distinct colors.  We now can plot the principal component score vectors.

```{r chunk46}
par(mfrow = c(1, 2))
plot(pr.out$x[, 1:2], col = Cols(nci.labs), pch = 19,
    xlab = "Z1", ylab = "Z2")
plot(pr.out$x[, c(1, 3)], col = Cols(nci.labs), pch = 19,
    xlab = "Z1", ylab = "Z3")
```

The resulting  plots are shown in Figure 12.17. On the whole, cell lines corresponding to a single cancer type do tend to have similar values on the first few
principal component score vectors. This indicates that cell lines from the same cancer type tend to have pretty similar gene expression levels.

We can obtain a summary of the proportion of variance explained (PVE) of the first few principal components using the `summary()` method for a `prcomp` object (we have truncated the printout):

```{r chunk47}
summary(pr.out)
```

Using the `plot()` function, we can also plot the variance explained by the first few principal components.

```{r chunk48}
plot(pr.out)
```

Note that the height of each bar in the bar plot is given by squaring the corresponding element of `pr.out$sdev`.
However, it is more informative to plot the PVE of each principal component (i.e. a scree plot) and the cumulative PVE of each principal component. This can be done with just a little work.

```{r chunk49}
pve <- 100 * pr.out$sdev^2 / sum(pr.out$sdev^2)
par(mfrow = c(1, 2))
plot(pve,  type = "o", ylab = "PVE",
    xlab = "Principal Component", col = "blue")
plot(cumsum(pve), type = "o", ylab = "Cumulative PVE",
    xlab = "Principal Component", col = "brown3")
```

(Note that the elements of `pve` can also be computed directly from the summary, `summary(pr.out)$importance[2, ]`, and the elements of
`cumsum(pve)` are given by
`summary(pr.out)$importance[3, ]`.)
The resulting plots are shown in Figure 12.18.

We see that together,
the first seven principal components explain around $40\,\%$ of the variance in the data. This is not a huge
amount of the variance. However, looking at the scree plot, we see that while each of the first seven principal components explain a substantial amount of  variance, there
is a marked decrease in the variance explained by further principal components. That is, there is an *elbow*   in the plot after approximately the seventh principal component.
This suggests that there may be little benefit to examining more than seven or so principal components (though even examining seven principal components may be difficult).


# Iris

```{r}
head(iris)
```

```{r}
plot_ly(iris, x=~Sepal.Length, y=~Sepal.Width, z=~Petal.Length, color=~Species)
```
```{r}
plot_ly(iris, x=~Sepal.Length, y=~Sepal.Width, z=~Petal.Width, color=~Species)
```

```{r}
plot_ly(iris, x=~Sepal.Length, y=~Sepal.Width, z=~Petal.Length, 
        frame=~round(Petal.Width*10), color=~Species)
```

```{r}
pr.out <- prcomp(iris[,-5], scale = TRUE)
pr.out
```

```{r}
ggplotly(ggbiplot(pr.out, scale = 0))
```

```{r}
ggplotly(ggbiplot(pr.out, scale = 0, groups=iris$Species, ellipse = TRUE, circle = TRUE))
```



```{r}
df_iris <- as.data.frame(pr.out$x) %>%
  mutate(Species=iris$Species)

df_iris_loadings <- as.data.frame(pr.out$rotation) %>%
  mutate(name=rownames(pr.out$rotation))

plot_ly(df_iris, x=~PC1, y=~PC2, color=~Species) %>%
  add_markers() %>%
  add_segments(data=df_iris_loadings,x=I(0),y=I(0),xend = ~PC1, yend = ~PC2,color=I('brown')) %>%
  add_text(data=df_iris_loadings,x = ~PC1, y = ~PC2,color=I('brown'), text=~name)
```


```{r}
plot_ly(df_iris, x=~PC1, y=~PC2, z=~PC3, color=~Species)
```


```{r}
df_iris_pve <- data.frame(PC=colnames(pr.out$rotation), 
                          PVE=100*pr.out$sdev^2/sum(pr.out$sdev^2)) %>%
  mutate(CumPVE=cumsum(PVE))

plot_ly(df_iris_pve, x=~PC) %>%
  add_markers(y=~PVE, color="PVE") %>%
  add_paths(y=~PVE, color="PVE") %>%
  add_markers(y=~CumPVE, color="CumPVE") %>%
  add_paths(y=~CumPVE, color="CumPVE")
```